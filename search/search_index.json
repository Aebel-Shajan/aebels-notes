{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my notes","text":"<p>super secret notes</p>"},{"location":"projects/","title":"Projects","text":""},{"location":"projects/#future","title":"Future","text":"<ul> <li> <p>vscode like minimap extension but for chrome</p> </li> <li> <p>data dashboard from fitbit, strong, samsung fit, google fit</p> </li> <li> <p>contents page for multiple documentation pages</p> </li> </ul>"},{"location":"projects/#present","title":"Present","text":"<ul> <li> <p>salamendar inverse kinematics</p> </li> <li> <p>habit tracker</p> </li> </ul>"},{"location":"projects/#past","title":"Past","text":"<ul> <li> <p>java spring boot, react js chatroom application</p> </li> <li> <p>subway surfers screen reader</p> </li> <li> <p>personal website</p> </li> <li> <p>three js car drifting game</p> </li> <li> <p>free code camp side panel contents extension</p> </li> <li> <p>c++ quantum circuit simulator</p> </li> </ul>"},{"location":"projects/#limbo","title":"Limbo","text":"<ul> <li> <p>three js + rapier js rocket league clone :(</p> </li> <li> <p>job application chrome extension</p> </li> </ul>"},{"location":"python/django/","title":"Django","text":"<p>Django docs</p> <p>Django Rest Framework docs</p>"},{"location":"python/django/#general","title":"General","text":"<ul> <li>Django is used to create static full stack server side website.</li> <li>Django rest framework is used to create apis</li> <li>Django makes use of the model, view, template design pattern.</li> <li>You can use shortcuts from django.shortcuts to cut down on code that gets repeated across multiple projects.</li> <li>You can use generic views to replace writing out views in full by hand.</li> </ul>"},{"location":"python/django/#making-model-changes","title":"Making model changes","text":"<ol> <li>Change your models (in models.py).</li> <li><code>python manage.py makemigrations</code> to create migrations for those changes</li> <li><code>python manage.py migrate</code> to apply those changes to the database.</li> </ol>"},{"location":"python/django/#runserver","title":"Runserver","text":"<p><code>python manage.py runserver</code></p>"},{"location":"python/pyspark/","title":"Pyspark","text":"<p>Following documentation from https://best-practice-and-impact.github.io/ons-spark/intro.html</p>"},{"location":"python/pyspark/#overview","title":"Overview","text":""},{"location":"python/pyspark/#sessions","title":"Sessions","text":"<ul> <li>Start a default session : <code>SparkSession.builder</code>/<code>spark_connect()</code></li> </ul> <pre><code>from pyspark.sql import SparkSession\nspark = (\n    SparkSession.builder.appName(\"default-session\")\n    .getOrCreate()\n)\n</code></pre> <ul> <li> <p>Stop sessions by using : <code>spark.stop()</code>/<code>spark_disconnect()</code></p> </li> <li> <p>Config <code>spark-defaults.conf</code> configuration file</p> </li> <li> <p>Two modes: local, cluster</p> </li> <li> <p>When using cdsw, spark config is already done for you when you launch the session.</p> </li> </ul>"},{"location":"python/pyspark/#data-types","title":"Data Types","text":"<ul> <li> <p>Types are inferred in Pyspark</p> </li> <li> <p>Import from <code>pyspark.sql.type</code> <pre><code># Structural types\nfrom pyspark.sql.types import StructType, StructField\n# String type\nfrom pyspark.sql.types import StringType\n# Numeric types\nfrom pyspark.sql.types import IntegerType, DecimalType, DoubleType\n# Date types\nfrom pyspark.sql.types import DateType, TimestampType\n</code></pre></p> </li> <li> <p>Example scheme: </p> </li> </ul> <pre><code>root\n|-- incident_number: string (nullable = true)\n|-- date_time_of_call: string (nullable = true)\n|-- cal_year: integer (nullable = true)\n|-- fin_year: string (nullable = true)\n</code></pre> <ul> <li> <p>Can read data from parquet files (scheme included) and csvs (not included, scheme inferred or provided like below) <pre><code>from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\nrescue_schema = StructType([\nStructField(\"incident_number\", StringType()),\nStructField(\"date_time_of_call\", StringType()),\nStructField(\"cal_year\", IntegerType()),\nStructField(\"fin_year\", StringType())\n])\n</code></pre></p> </li> <li> <p>Column methods <code>.cast()</code> or <code>.astype()</code> to change type of column</p> </li> </ul>"},{"location":"python/pyspark/#create-df-manually","title":"Create DF manually","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession, functions as F\n\nspark = (SparkSession.builder.master(\"local[2]\")\n         .appName(\"create-DFs\")\n         .getOrCreate())\n\nseed_no = 100\nrandom_numbers = (spark.range(5)\n                  .withColumn(\"rand_no\", F.rand(seed_no)))\n\nrandom_numbers.show()\n</code></pre>"},{"location":"python/pyspark/#introduction-to-pyspark","title":"Introduction to Pyspark","text":"<p>PySpark DataFrames are processed on the Spark cluster. This is a big pool of linked machines, called nodes. PySpark DataFrames are distributed into partitions, and are processed in parallel on the nodes in the Spark cluster. You can have much greater memory capacity with Spark and so is suitable for big data.</p>"}]}